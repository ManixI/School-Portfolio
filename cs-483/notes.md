# CS-483 Notes

## Find-S
 - Explores entire hypothesis space of $2^{2^{num features}}$
 - tries to find the most specific hypothisis that fits the criteria
 - Example:
 
 |Wag Tail|Hops |Four Legs|Is Dog|
 |:------:|:---:|:-------:|:----:|
 |False   |False|True     |False |
 |True    |False|True     |True  |
 |True    |True |True     |False |
 |False   |False|True     |True  |

 Start: ( \_: nothing, ?: anything) \
 \
 0. (\_|\_, \_|\_, \_|\_) Never a Dog (start state)
 1. Ignore first row, not a dog so not relevant
 2. (True, False, True) Update based on first true target
 3. Ignore 3rd row, not a dog
 4. (?, False, True) Update based on 4th row
 
 The hypothesis generated by find-s (?, False, True) will predict the following

 |Wag Tail|Hops |Four Legs|Is Dog|Prediction|Result|
 |:------:|:---:|:-------:|:----:|:--------:|:----:|
 |False   |False|True     |False |True      |FP    |
 |True    |False|True     |True  |True      |TP    |
 |True    |True |True     |False |False     |TN    |
 |False   |False|True     |True  |True      |TP    |


 Accuracy: 3/4 \
 Precision: 2/3 \
 Recall: 1


## Naive Bayes
 - $
 - P(A|B) =$ $P(A) * P(B|A) \over P(B)$
 - Leplah Smoothing:
     + 
 - example:
 
 |Tail |Feathers|Beak |Class  |
 |:---:|:------:|:---:|:-----:|
 |False|True    |True |Bird   |
 |True |False   |False|Reptile|
 |True |False   |False|Mammal |

 \<False, False, True> = ? \
 est numerator = p(class) * p(!Tail|class) * p(Feathers|class) * p(Beak|class) \
 est. numerator for bird = ~ 0.0002 \
 est. numerator for rept = ~ 0.0001 \
 est. numerator for mamm = ~ 0.0001 \
 \
 prob bird = 0.0002 / 0.0004 = 1/2 \
 
## Logistic Regression
 - a
 
## Confusion Matrix
 - PR Curve, graph of precision vs recall at different thresholds

## Pipeline
 - debug by calling pre-processor manually for full stack-trace
 
### Example Code

```python
    # some code here
    # more here
```

## Entropy and Gini
 - Measure the predictability of data
 - Entropy specifically measures number of bit needed to encode an event
 - Gini impurity: $1 - (\sqrt{prob A} + \sqrt{prob B} + ...)$ (purity is complement of this)
 - Lower impurity is better
 - useful for building tree, split that reduces entropy the most is the best split
 

## Decisin Trees
 - Random Forest and Gradient Boost 
 - Issue with over-fitting, solve by trimming trees and making a bunch
 - Random Forest:
 	+ max depth 
 	+ bootstrapping (sample rows), not good for lots of data
 	+ bagging (sample w/o replacement), not good for lots of data
 	+ feature selection, usually use sqrt of num features
 	+ num trees, trade accuracy for performance
 - Gradient Boosting (GBTree):
 	+ should slot in anywhere random forest works
 	+ First tree predicts answer, second tree predicts how wrong first tree is, third predicts second, ect.
 	+ residual (how wrong a tree is, or difference from correct answer): inverse of predicted
 
 ### Example Tree Code:
 
```python
    class SelectColumns(BaseEstimator, TransformerMixin):
        # pass the function we want to apply to the column 'SalePriceâ€™
        def __init__( self, columns ):
            self.columns = columns
        # don't need to do anything
        def fit( self, xs, ys, **params ):
            return self
        #actually perform the selection
        def transform( self, xs ):
            return xs[ self.columns ]

    forest = RandomForestClassifier(criterion='entropy')

    # dummy values, is there a better way?
    steps = [
        ( 'column_select', SelectColumns(['Overall Qual']) ),
        ( 'random_forest', RandomForestClassifier(criterion='gini') ),
    ]
    pipe = Pipeline( steps )
    pipe.fit(train_x, train_y )

    grid = { 
        'column_select__columns': list_tests,
        'random_forest__n_estimators':range(75,105,2),
        'random_forest__criterion':['gini', 'entropy'],
        'random_forest__criterion':['entropy'],
        'random_forest__max_depth':range(5,15)           
    }
        
    search = GridSearchCV( pipe, grid, scoring = 'r2', n_jobs = -1, cv=5 )
    search.fit( self.xs, self.ys )

    # scoring
    best_estimator = search.best_estimator_
    best_params = best_estimator.get_params
    score = search.best_score_

    # predicting
    guess = search.predict(test_x)

```

 ## Clustering:
 - Normalize all data to properly cluster when some parts of data have lower variance the others
 - use MinMaxScaler()
 - scored silhouette coefficient
 	+ A average inter-cluster distance, small is good
 	+ B distance between two closest clusters, average distance of each point in A to each point in B, large is good
 	+ $B - A \over max(B, A)$ (breaks for single point, just make score 1)
 	+ normalizes score
 - KMeans
 	+ non-deterministic
 	+ start with n random centroides that are built into clusters
 	+ groups by euclidean distance to centroid (other distances not guaranteed to end)
 	+ then move centroid to center of points assigned to it
 	+ repeat last two steps until point assignments no longer change (centroid no longer moves)
 	+ goal is to find centroids with lowest in-cluster distance
 	+ O complexity (linear)
 - Agglomerative
 	+ Deterministic
 	+ every instances starts in own cluster
 	+ group two closest clusters into single cluster
 	+ can use any distance metric (affinity)
 	+ repeat until desired number of clusters are created
 	+ center of cluster is determined by linkage:
 		* centroid linkage / average linkage
 		* simple linkage, measures distance to nearest edge
 		* ward linkage, pair by measuring smallest increase to inter-cluster distance
        * complete linkage, distance between furthest points (never best option)
        * DB-SCAN, differentiates between core and peripheral points in a cluster
 	+ better for weird shapes such as circle in circle then KMeans
 	+ can build to distance threshold instead of num clusters
 	+ Dendragram (culster history over time)
 	+ very slow, $O*n * O(n-1) \over 2$ pairs ($O^2$ complexity)
 - [affinities] (distance measures)
 	+ Euclidean (L1 norm): strait line difference $d_{euclid}(a,b) = (\sum_{i=0}^{k-1}(a_i-b_i)^2)^{1/2}$
    + penalizing based on distance from 0 (L2 reguralization) is LASSO - leas absolute shrinkage and selection operator. This is preferring more normalized data
 	+ Manhattan (L2 norm): distance blocks, $|X_{2}-X_{1}| + |Y_{2}-Y_{1}| + |Z_{2}-Z_{1}|$, good when more then 2-3 dimensions
    + penalizing based on each feature's distance from 0 (L1 regurgitation) is Ridge. This is preferring a model where every feature is 0
    + Both LASSO and Ridge are different ways of penalizing unimportant features
 	+ MnCofskey distance: class including euclidean and Manhattan, smoothly change between them, $P_{norm}(a,b) = (\sum_{i=0}^{k-1}(a_i-b_i)^p)^{1/p}$
 	+ Cos[^1] Dissimilarity: good for vectors, strings. Measures direction not position, bounded -1 - 1, dot product[^2] of normalized data points
    + this is measuring if two vectors are pointing the same direction (1), opposite directions (-1), or how different their directions are (between -1 and 1) 
    + You take toe complement of this to get similarity
    + example of this is testing if a tweet is positive of negative, degree of positivity (re. num emogies) doesn't matter
    
[^1]: shadow of triangle
[^2]: $a_{vec} \dot b_{vec} = \sum_{i=0}^{k-1}a_i * b_i$ 
    

 ### Example Code:
 
```python
    #NOTE: this dose not use a pipeline
    k_range = range(2,15)
    
    # get all rows with matching type 1
    type_data = data.loc[data['Type 1'] == typ]

    # normalize data
    scaler = MinMaxScaler()
    scale = scaler.fit_transform(type_data[scale_elements])
    type_data_scale = pd.DataFrame(scale, columns=scale_elements)

    best_k = 2
    best_score = 0
    # find best number of clusters by looping over possible k values
    for k in k_range:
        cluster = KMeans(k,random_state=0).fit(type_data_scale).labels_
        score = silhouette_score(type_data_scale, cluster,metric="euclidean",random_state=0)
        # keep best score
        if best_score < score:
            best_score = score
            best_k = k
            # store set of best cluster to print later
            type_df = pd.DataFrame()
            type_df['data_index'] = type_data.index.values
            type_df['cluster'] = cluster
    print("best num of clusters: "+str(best_k))
    print("best score: "+str(best_score))
```

## Nearest Neighbor
 - classify data point same as closest data point
 - uses various distances (see[affinities][[^2])
 - very simple
 - works for regression or classification
 - like clustering, but supervised
 - can take average of several ex. 2-nearest neighbor, 3-nearest neighbor
 - can also exclude neighbors that are too far away
 - Good if you need to manually classify a subset of data, then use this to generate a lot more very similar data from larger set to use for other type of classification (semi-supervised)

## [Bag of Words][bow link]
- bag is set with multiplicity (count)
- fuzzy set, can be partial member
- use dictionary to determine important terms in doc (TF-IDF)
- turn documents (set of lists) into vectors, each doc is bag of terms in dict
- can then pass as numeric data to classifiers
- basically get the reverse index used for TF-IDF and use that to turn each page into a vector
- e.g. the document: "Toad, Toad," cried frog becomes <2,1,1,1,> 
- if you add: Wake up, it's a new day. they become <2,1,1,1,0,0,0,0,0,>, <0,0,0,0,1,1,1,1,1,1>
- Vocab is words that are elements in a vector, the vocab can be stopped, stemmed, and not updated
    + stemming: removing prefixes, plurals, other variations
    + stopping: removing very common words (usually ones in more then 80% of all docs)
    + converting everything to lowercase almost always ideal
    + can also include punctuation
    + other misc. modifiers that can test well in pipe:
        * replace numbers with generic NUMBER token
        * replace negatives with generic NEGATIVE token
        * shrink repeated letters to max of 2-3
        * strip accents
        * n-grams
        * min and max doc frequency
- N-grams to track word order, treat n words as single word (track single words as well)
- 2-3 is best, after that significant performance penalties for little gain
- use `CountVectorizer()`


## Cross Validation
 - between 5-10 folds beat (empirically best)
 - leave one out also useful
 - remove bias in train-test split
 - do before feature selection, not after (can use to find spurious features when it's only good in one fold)
 - `GridSearchCV(cv=folds)`

 ## Metrics
 - Precision:
 - Recall:
 - precision and recall have inverse collation
 


 # Apendix
 ## Python imports for code
 
 ```python
    import math
    import warnings
    import pandas as pd
    import numpy as np
    from functools import lru_cache
    from sklearn.model_selection import train_test_split
    from sklearn.model_selection import GridSearchCV
    from sklearn.pipeline import Pipeline
    from sklearn.base import TransformerMixin, BaseEstimator
    from sklearn.metrics import r2_score
    from sklearn.metrics import accuracy_score
    from sklearn.linear_model import LinearRegression
    from sklearn.compose import TransformedTargetRegressor
    from sklearn.tree import DecisionTreeClassifier, plot_tree
    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
```

## Pipeline Examples:

### Bag of Words Pipeline
```python
    # this will be slow
    class Preprocessor(BaseEstimator, TransformerMixin):
        def fit(self, xs, ys=None): return self
        def transform(self, xs):
            # remove all HTML tags and lowercase strings
            # remove specificly line breaks, ect.
            def de_tag(t): return re.sub('<br />',' ',t)
            def drop_quote_and_hyphen(t): return re.sum(r"\'|\-", '', t)
            def drop_puncuation(t): return re.sub('\W', ' ', t)
            def shrink_spaces(t): return e.sub('\s+', ' ', t)

            transformed = sx.str.lower()
            transformed = transformed.apply(de_tag)
            transformed = transformed.apply(drop_quote_and_hyphen)
            transformed = transformed.apply(drop_puncuation)
            transformed = transformed.apply(shrink_spaces)

            return transformed

    vectorizer = \
        CountVectorixer(
            max_df=0.5,
            min_df=3,
            strip_accents='unicode',
            max_features=10000,
            ngram_range=(1,4)
        )

    steps = [
        ('tokenize',Preprocessor()),
        ('vectorize',vectorizer),
        ('normalize',Normalizer(norm='l1')), #peanalize words that don't occure frequently
        ('classify',LogisticRegression())
    ]

    pipe = Pipeline(steps)
    #pipe.fit_transform(xs)
    pipe.fit(xs, xy)
    # ect...
```

### Clustering Pipeline
```python
    # func to get best num clusters in pipeline
    def optomize_n_clusters(r, data):
        best_choice = r.start
        best_silhouette = float('-inf')

        for n_clusters in r:
            score = silhouette_coefficent(n_clusters, data)

            if score > best_silhouette:
                best_silhouette = score
                best_choice = n_clusters

        return best_choice, best_silhouette
    # example, normally goes in the stages
    optomize_n_clusters(range(10,20), data)

    stages = [
        ('scale', MinMaxScaler()),
        ('cluster', KMeans(2, random_state=0)),
        ]
    pipe = Pipeline(stages)
    # Don't train-test split for unsupervided
    pipe.fit(xs)
    pipe.predict(new_xs)

    
```

## Useful Code
[bow link] Bag of Words

    delete - and ' in words:
    `document = re.sum(r'(\')|(\-)', '', document, re.UNICODE)`

    replace non-letters with spaces: 
    `document = re.sub('([^\w\'\-])', ' ', document, re.UNICODE)`

    shrink all whitespace to single space:
    `document = re.sub('\s+', '', document, re.UNICODE)`

## Terms
 - Feature: column in data set
 - Instance: row in data set
 - Target: what we want to predict
 - Supervised: classifying data with a target to predict (nearest neighbor, regression, trees)
 - Unsupervised: classifying without a target (clustering)
 - Accuracy: Not good on it's own, $P_{true} + N_{true} \over P_{all} + N_{all}$
 - Precision: How careful you are, $P_{true} \over P_{all}$
 - Recall: Catch rate/thoroughness, $P_{true} \over P_{true} + N_{false}$
 - F-Score: Combination of precision and recall,  $2 * Precision * Recall \over Precision + Recall$
 - $F_b-Score$: Like f-score but can be weighted for precision or recall (higher B favors precision), (1+B^2) * Precision * Recall \over (B^2 * Precision) + Recall$
 - Harmonic Mean: $n \over  (1/x_1) + (1/x_2) + (1/x_3) ... $
 - Hypothesis: Function that potentially represent the combination of important features, often written as truth table
 - Hypothesis Space: All possible Hypothesis